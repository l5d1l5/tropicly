{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data acquisition\n",
    "This *Jupyter* notebook comprises the required core and auxiliary data downloads for my master thesis. All downloads are stored locally in a directory tree called ***data*** within the notebooks root directory. Be sure that you have enough space on your hard disk because these downloads need it. Further, you should have a fast web access in order to download the required files quickly. The proceeding notebook is a top down approach, therefore you should execute code cells in a top down manner. If you execute code cells in a arbitrary order it leads to unrecognized exceptions and errors. So, for your own sake don't do it please. Moreover, it is not checked if you already downloaded the data yet. Hence, executing the code snippets twice will lead to re-download of the entire datasets.\n",
    "\n",
    "My master thesis has the emphasis deforestation and deforestation drivers on global, continental and local scale in the tropical zone. Therefore, the downloads will be filtered in the extent of this zone which covering a latitudinal area between 23.43&deg; North and 23.43&deg; South (WGS84). This notebook comprises the following sections:\n",
    "\n",
    "[**1.1. Preparation**](#1.1.-Preparation) contains all required initial steps like importing crucial standard library modules and construction of the directory tree for storing the downloaded datasets. The code snippets of this section are fundamental and if you refuse to execute them the proceeding code cells will run into fatal errors.\n",
    "\n",
    "[**1.2. Core data**](#1.2.-Core-data)\n",
    "\n",
    "[**1.3. Auxiliary data**](#1.3.-Auxiliary-data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Preparation\n",
    "As the first step we must import all necessary *Python* standard library modules for the data download and filtering. In detail the following modules are required:\n",
    "- ***collections.namedtuple***\n",
    "- ***IPython.display.clear_output*** a module method to clear the output of an arbitrary code cell\n",
    "- ***urllib.request*** a module to open and download urls\n",
    "- ***os*** a module to use operating system dependent functionality\n",
    "- ***re*** a module for applying regular expressions\n",
    "- ***threading***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "from IPython.display import clear_output\n",
    "import urllib.request\n",
    "import os\n",
    "import re\n",
    "import threading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we create with the following code cell the ***data*** directory tree among the root folder.\n",
    "- **data**\n",
    "    - **core** the entire data from [Section 1.2.](#1.2.-Core-data)\n",
    "        - **gfc** data from [Section 1.2.1.](#1.2.1.-Global-Forest-Change)\n",
    "        - **gl30** data from [Section 1.2.2.](#1.2.2.-GlobalLand30)\n",
    "        - **gc** data from [Section 1.2.3.](#1.2.3.-GlobCover)\n",
    "    - **auxiliary** the entire data from [Section 1.3.](#1.3.-Auxiliary-data)\n",
    "    - **urls** URLs from different sources for validation \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created:\tdata\n",
      "Created:\tdata/urls\n",
      "Created:\tdata/core\n",
      "Created:\tdata/core/gfc\n",
      "Created:\tdata/core/gl30\n",
      "Created:\tdata/core/gc\n",
      "Created:\tdata/auxiliay\n"
     ]
    }
   ],
   "source": [
    "Directories = namedtuple('Directories', 'root urls core gfc gl30 gc auxiliary'.split())\n",
    "directories_data = 'data data.urls data.core data.core.gfc data.core.gl30 data.core.gc data.auxiliay'\n",
    "\n",
    "# os compatibility replace \".\" with os dependent path seperator\n",
    "dirs = Directories(*re.sub(r'\\.', os.sep, directories_data).split())\n",
    "\n",
    "for directory in dirs:\n",
    "    try:\n",
    "        os.mkdir(directory)\n",
    "        print('Created:\\t{}'.format(directory))\n",
    "    except OSError as e:\n",
    "        print('Error:\\t{} {}'.format(directory, e.strerror))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Core data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1. Global Forest Change\n",
    "[**Global Forest Change 2000-2012 (V1.0)**](https://earthenginepartners.appspot.com/science-2013-global-forest/download_v1.0.html)\n",
    "- [**Treecover2000**](http://commondatastorage.googleapis.com/earthenginepartners-hansen/GFC2013/treecover2000.txt)\n",
    "- [**Gain**](http://commondatastorage.googleapis.com/earthenginepartners-hansen/GFC2013/gain.txt)\n",
    "- [**Lossyear**](http://commondatastorage.googleapis.com/earthenginepartners-hansen/GFC2013/lossyear.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treecover2000:\t504 URLs\n",
      "Gain:\t\t504 URLs\n",
      "Lossyear:\t504 URLs\n"
     ]
    }
   ],
   "source": [
    "base_url = 'http://commondatastorage.googleapis.com/earthenginepartners-hansen/GFC2013/'\n",
    "\n",
    "treecover_urls = urllib.request.urlopen(base_url + 'treecover2000.txt').read().decode().splitlines()\n",
    "gain_urls = urllib.request.urlopen(base_url + 'gain.txt').read().decode().splitlines()\n",
    "lossyear_urls = urllib.request.urlopen(base_url + 'lossyear.txt').read().decode().splitlines()\n",
    "\n",
    "print('Treecover2000:\\t{} URLs\\nGain:\\t\\t{} URLs\\nLossyear:\\t{} URLs'\n",
    "      .format(len(treecover_urls), len(gain_urls), len(lossyear_urls)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288 URLs are not in bounds between 30 North and 20 South.\n",
      "Filtered URLs contains 3 * 216 elements.\n"
     ]
    }
   ],
   "source": [
    "def is_in_extent(coord: int, orient: str, north_limit: int, south_limit: int):\n",
    "    if orient.lower() == 'n':\n",
    "        return coord <= north_limit\n",
    "    elif orient.lower() == 's':\n",
    "        return coord <= south_limit\n",
    "    return False\n",
    "\n",
    "# http://commondatastorage.googleapis.com/earthenginepartners-hansen/GFC2013/Hansen_GFC2013_lossyear_00N_030W.tif\n",
    "regex = re.compile(r\"\"\"\n",
    "                        (?:\\w+_){3}  # supress group consumption, alphanumeric char one or more times followed by underline, match group three times  \n",
    "                        (?P<coord>\\d{2})  # named group consumption, digit two times\n",
    "                        (?P<orient>S|N)  # named group consumption, S or N\n",
    "                    \"\"\", re.VERBOSE)\n",
    "\n",
    "# [(treecover, gain, lossyear), ...]\n",
    "raw_urls = list(zip(treecover_urls, gain_urls, lossyear_urls))\n",
    "filtered_urls = []\n",
    "\n",
    "for urls in raw_urls:\n",
    "    coord, orient = regex.search(urls[0]).groups()\n",
    "    if is_in_extent(int(coord), orient, 30, 20):\n",
    "        filtered_urls.append(urls)\n",
    "\n",
    "print('{} URLs are not in bounds between {} North and {} South.'.format(len(raw_urls) - len(filtered_urls), 30, 20))\n",
    "print('Filtered URLs contains 3 * {} elements.'.format(len(filtered_urls)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOWNLOADED 648 OF 648 FILES\n"
     ]
    }
   ],
   "source": [
    "def open_urls(*args):\n",
    "    response = [urllib.request.urlopen(url) for url in args]\n",
    "    return response\n",
    "\n",
    "def write_response(out_path: str, *args):\n",
    "    for response in args:\n",
    "        filename = response.url.split('/')[-1]\n",
    "        with open(out_path + os.sep + filename, 'wb') as dst:\n",
    "            dst.write(response.read())\n",
    "        print('DOWNLOADED {}\\nTO {}'.format(response.url, out_path + os.sep + filename))\n",
    "\n",
    "def worker(out_path: str, urls: list):\n",
    "    response = open_urls(*urls)\n",
    "    write_response(out_path, *response)\n",
    "\n",
    "finished = 0\n",
    "for idx in range(0, len(filtered_urls), 3):\n",
    "    current_urls = filtered_urls[idx:idx + 3]\n",
    "    threads = []\n",
    "    for urls in current_urls:\n",
    "        thread = threading.Thread(target=worker, args=(dirs.gfc, urls))\n",
    "        thread.start()\n",
    "        threads.append(thread)\n",
    "    [thread.join() for thread in threads]\n",
    "    clear_output()\n",
    "    finished += len(current_urls) * 3\n",
    "    print('DOWNLOADED {} OF {} FILES'.format(finished, len(filtered_urls) * 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2. GlobalLand30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3. GlobCover"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Auxiliary data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
