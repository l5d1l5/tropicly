{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data acquisition\n",
    "This *Jupyter* notebook comprises the required core and auxiliary data downloads for my master thesis. All downloads are stored locally in a directory tree called ***data*** within the notebooks root directory. Be sure that you have enough space on your hard disk because these downloads need it. Further, you should have a fast web access in order to download the required files quickly. The proceeding notebook is a top down approach, therefore you should execute code cells in a top down manner. If you execute code cells in a arbitrary order it leads to unrecognized exceptions and errors. So, for your own sake don't do it please. Moreover, it is not checked if you already downloaded the data yet. Hence, executing the code snippets twice will lead to re-download of the entire datasets.\n",
    "\n",
    "My master thesis has the emphasis deforestation and deforestation drivers on global, continental and local scale in the tropical zone. Therefore, the downloads will be filtered in the extent of this zone which covering a latitudinal area between 23.43&deg; North and -23.43&deg; South (WGS84). This notebook comprises the following sections:\n",
    "\n",
    "[**Preparation**](#Preparation) contains all required initial steps like importing crucial standard library modules and construction of the directory tree for storing the downloaded datasets. The code snippets of this section are fundamental and if you refuse to execute them the proceeding code cells will run into fatal errors.\n",
    "\n",
    "[**Core data**](#Core-data)\n",
    "\n",
    "[**Auxiliary data**](#Auxiliary-data) <font color='red'>**Not done yet**</font>\n",
    "\n",
    "[**Result**](#Result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "As the first step we must import all necessary *Python* standard library modules for the data download and filtering. In detail the following modules are required:\n",
    "- ***collections.namedtuple*** a module to create tuple-like objects that have fields accessible by attribute lookup as well as being indexable and iterable\n",
    "- ***IPython.display.clear_output*** a module method to clear the output of an arbitrary code cell\n",
    "- ***urllib.request*** a module to open and download urls\n",
    "- ***os*** a module to use operating system dependent functionality\n",
    "- ***re*** a module for applying regular expressions\n",
    "- ***threading*** a module that provides a multi-threading API\n",
    "- ***zipfile*** a module that provides tools for ZIP archives\n",
    "- ***geopandas*** a powerful and feature rich package for manipulating geo-vector files like shp, geojson etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "from IPython.display import clear_output\n",
    "import urllib.request\n",
    "import os\n",
    "import re\n",
    "import threading\n",
    "import zipfile\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we create with the following code cell the ***data*** directory tree among the root folder.\n",
    "- **data**\n",
    "    - **core** the entire data from [Section 1.2](#Core-data)\n",
    "        - **gfc** data from [Section 1.2.1](#Global-Forest-Change)\n",
    "        - **gl30** data from [Section 1.2.2](#GlobalLand30)\n",
    "        - **gc** data from [Section 1.2.3](#GlobCover)\n",
    "    - **auxiliary** the entire data from [Section 1.3](#Auxiliary-data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:\tdata File exists\n",
      "Error:\tdata/core File exists\n",
      "Error:\tdata/core/gfc File exists\n",
      "Error:\tdata/core/gl30 File exists\n",
      "Error:\tdata/core/gc File exists\n",
      "Error:\tdata/auxiliary File exists\n",
      "Error:\tdata/masks File exists\n"
     ]
    }
   ],
   "source": [
    "Directories = namedtuple('Directories', 'root core gfc gl30 gc auxiliary masks'.split())\n",
    "directories_data = 'data data.core data.core.gfc data.core.gl30 data.core.gc data.auxiliary data.masks'\n",
    "\n",
    "# os compatibility replace \".\" with os dependent path separator\n",
    "dirs = Directories(*re.sub(r'\\.', os.sep, directories_data).split())\n",
    "\n",
    "for directory in dirs:\n",
    "    try:\n",
    "        os.mkdir(directory)\n",
    "        print('Created:\\t{}'.format(directory))\n",
    "    except OSError as e:\n",
    "        print('Error:\\t{} {}'.format(directory, e.strerror))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Forest Change\n",
    "[**Global Forest Change 2000-2012 (V1.0)**](https://earthenginepartners.appspot.com/science-2013-global-forest/download_v1.0.html)\n",
    "- [**Treecover2000**](http://commondatastorage.googleapis.com/earthenginepartners-hansen/GFC2013/treecover2000.txt)\n",
    "- [**Gain**](http://commondatastorage.googleapis.com/earthenginepartners-hansen/GFC2013/gain.txt)\n",
    "- [**Lossyear**](http://commondatastorage.googleapis.com/earthenginepartners-hansen/GFC2013/lossyear.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treecover2000:\t504 URLs\n",
      "Gain:\t\t504 URLs\n",
      "Lossyear:\t504 URLs\n"
     ]
    }
   ],
   "source": [
    "base_url = 'http://commondatastorage.googleapis.com/earthenginepartners-hansen/GFC2013/'\n",
    "\n",
    "treecover_urls = urllib.request.urlopen(base_url + 'treecover2000.txt').read().decode().splitlines()\n",
    "gain_urls = urllib.request.urlopen(base_url + 'gain.txt').read().decode().splitlines()\n",
    "lossyear_urls = urllib.request.urlopen(base_url + 'lossyear.txt').read().decode().splitlines()\n",
    "\n",
    "print('Treecover2000:\\t{} URLs\\nGain:\\t\\t{} URLs\\nLossyear:\\t{} URLs'\n",
    "      .format(len(treecover_urls), len(gain_urls), len(lossyear_urls)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288 URLs are not in bounds between 30 North and -20 South.\n",
      "Filtered URLs contains 3 * 216 elements.\n"
     ]
    }
   ],
   "source": [
    "def is_in_extent(coord: int, orient: str, north_limit: int, south_limit: int):\n",
    "    if orient.lower() == 'n':\n",
    "        return coord <= north_limit\n",
    "    elif orient.lower() == 's':\n",
    "        return coord <= south_limit\n",
    "    return False\n",
    "\n",
    "# http://commondatastorage.googleapis.com/earthenginepartners-hansen/GFC2013/Hansen_GFC2013_lossyear_00N_030W.tif\n",
    "regex = re.compile(r\"\"\"\n",
    "                        (?:\\w+_){3}  # supress group consumption, alphanumeric char one or more times followed by underline, match group three times  \n",
    "                        (?P<coord>\\d{2})  # named group consumption, digit two times\n",
    "                        (?P<orient>S|N)  # named group consumption, S or N\n",
    "                    \"\"\", re.VERBOSE)\n",
    "\n",
    "# [(treecover, gain, lossyear), ...]\n",
    "raw_urls = list(zip(treecover_urls, gain_urls, lossyear_urls))\n",
    "filtered_urls = []\n",
    "\n",
    "for urls in raw_urls:\n",
    "    coord, orient = regex.search(urls[0]).groups()\n",
    "    if is_in_extent(int(coord), orient, 30, 20):\n",
    "        filtered_urls.append(urls)\n",
    "\n",
    "print('{} URLs are not in bounds between {} North and {} South.'\n",
    "      .format(len(raw_urls) - len(filtered_urls), 30, -20))\n",
    "print('Filtered URLs contains 3 * {} elements.'.format(len(filtered_urls)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOWNLOADED 648 OF 648 FILES\n"
     ]
    }
   ],
   "source": [
    "def open_urls(*args):\n",
    "    response = [urllib.request.urlopen(url) for url in args]\n",
    "    return response\n",
    "\n",
    "\n",
    "def write_response(out_path: str, *args):\n",
    "    for response in args:\n",
    "        filename = response.url.split('/')[-1]\n",
    "        with open(out_path + os.sep + filename, 'wb') as dst:\n",
    "            dst.write(response.read())\n",
    "        print('DOWNLOADED {}\\nTO {}'.format(response.url, out_path + os.sep + filename))\n",
    "\n",
    "\n",
    "def worker(out_path: str, urls: list):\n",
    "    response = open_urls(*urls)\n",
    "    write_response(out_path, *response)\n",
    "\n",
    "\n",
    "finished = 0\n",
    "for idx in range(0, len(filtered_urls), 3):\n",
    "    current_urls = filtered_urls[idx:idx + 3]\n",
    "    threads = []\n",
    "    for urls in current_urls:\n",
    "        thread = threading.Thread(target=worker, args=(dirs.gfc, urls))\n",
    "        thread.start()\n",
    "        threads.append(thread)\n",
    "    [thread.join() for thread in threads]\n",
    "    clear_output()\n",
    "    finished += len(current_urls) * 3\n",
    "    print('DOWNLOADED {} OF {} FILES'.format(finished, len(filtered_urls) * 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GlobalLand30\n",
    "\n",
    "[GL30 mask](http://globallandcover.com/document/globemapsheet.zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted the following files ['GlobeMapSheet.dbf', 'GlobeMapSheet.prj', 'GlobeMapSheet.sbn', 'GlobeMapSheet.sbx', 'GlobeMapSheet.shp', 'GlobeMapSheet.shp.xml', 'GlobeMapSheet.shx']\n",
      "Removed archive data/masks/globemapsheet.zip\n"
     ]
    }
   ],
   "source": [
    "gl30_mask_url = 'http://globallandcover.com/document/globemapsheet.zip'\n",
    "gl30_mask_path = dirs.masks + os.sep + gl30_mask_url.split('/')[-1]\n",
    "\n",
    "with zipfile.ZipFile(gl30_mask_path, 'r') as src:\n",
    "    to_extract = src.namelist()\n",
    "    src.extractall(path=dirs.masks)\n",
    "    \n",
    "print('Extracted the following files {}'.format(to_extract))\n",
    "os.remove(gl30_mask_path)\n",
    "print('Removed archive {}'.format(gl30_mask_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "495 Tiles are not in bounds between 23.43 North and -23.43 South.\n",
      "File filter contains 358 elements.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NS</th>\n",
       "      <th>UTMZONE</th>\n",
       "      <th>ROW</th>\n",
       "      <th>CONTINENT</th>\n",
       "      <th>REMARK</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>S</td>\n",
       "      <td>48</td>\n",
       "      <td>10</td>\n",
       "      <td>Asia</td>\n",
       "      <td>S48_10</td>\n",
       "      <td>POLYGON ((108 -15.00000003, 101.99999988 -15.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>S</td>\n",
       "      <td>50</td>\n",
       "      <td>10</td>\n",
       "      <td>Asia</td>\n",
       "      <td>S50_10</td>\n",
       "      <td>POLYGON ((119.99999988 -15.00000003, 114.00000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>S</td>\n",
       "      <td>43</td>\n",
       "      <td>5</td>\n",
       "      <td>Asia</td>\n",
       "      <td>S43_5</td>\n",
       "      <td>POLYGON ((78.00000012 -9.999999989999999, 72 -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>S</td>\n",
       "      <td>48</td>\n",
       "      <td>5</td>\n",
       "      <td>Asia</td>\n",
       "      <td>S48_5</td>\n",
       "      <td>POLYGON ((108 -9.999999989999999, 101.99999988...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>S</td>\n",
       "      <td>49</td>\n",
       "      <td>5</td>\n",
       "      <td>Asia</td>\n",
       "      <td>S49_5</td>\n",
       "      <td>POLYGON ((114.00000012 -9.999999989999999, 108...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   NS  UTMZONE  ROW CONTINENT  REMARK  \\\n",
       "37  S       48   10      Asia  S48_10   \n",
       "38  S       50   10      Asia  S50_10   \n",
       "39  S       43    5      Asia   S43_5   \n",
       "40  S       48    5      Asia   S48_5   \n",
       "41  S       49    5      Asia   S49_5   \n",
       "\n",
       "                                             geometry  \n",
       "37  POLYGON ((108 -15.00000003, 101.99999988 -15.0...  \n",
       "38  POLYGON ((119.99999988 -15.00000003, 114.00000...  \n",
       "39  POLYGON ((78.00000012 -9.999999989999999, 72 -...  \n",
       "40  POLYGON ((108 -9.999999989999999, 101.99999988...  \n",
       "41  POLYGON ((114.00000012 -9.999999989999999, 108...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gl30_mask_shp = [item for item in os.listdir(dirs.masks) if bool(re.match(r'.+\\.shp$', item))][0]\n",
    "gl30_mask_path = dirs.masks + os.sep\n",
    "\n",
    "gl30_mask = gpd.read_file(gl30_mask_path + gl30_mask_shp).cx[:,-23.43:23.43]\n",
    "gl30_file_filter = [\n",
    "    '_'.join([items[0] + str(items[1]).zfill(2), str(items[2]).zfill(2)])\n",
    "    for items in zip(gl30_mask.NS, gl30_mask.UTMZONE, gl30_mask.ROW)\n",
    "                   ]\n",
    "\n",
    "print('{} Tiles are not in bounds between {} North and {} South.'\n",
    "      .format(853 - len(gl30_mask), 23.43, -23.43))\n",
    "print('File filter contains {} elements.'.format(len(gl30_file_filter)))\n",
    "gl30_mask.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "regex_id = re.compile(r'(?P<id>(?:N|S)\\d{2}_\\d{2})')\n",
    "regex_ext = re.compile(r'.+\\.tif')\n",
    "\n",
    "count = 0\n",
    "for item in os.listdir(dirs.gl30):\n",
    "    match = regex_id.search(item).group('id')\n",
    "    if match not in gl30_file_filter:\n",
    "        os.remove(dirs.gl30 + os.sep + item)\n",
    "    else:\n",
    "        with zipfile.ZipFile(dirs.gl30 + os.sep + item) as src:\n",
    "            to_extract = [ele for ele in src.namelist() if bool(regex_ext.match(ele))][0]\n",
    "            src.extract(to_extract, path=dirs.gl30)\n",
    "        os.rename(dirs.gl30 + os.sep + to_extract, dirs.gl30 + os.sep + to_extract.split(os.sep)[-1])\n",
    "        os.rmdir(dirs.gl30 + os.sep + to_extract.split(os.sep)[0])\n",
    "        os.remove(dirs.gl30 + os.sep + item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GlobCover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOWNLOADED http://due.esrin.esa.int/files/Globcover2009_V2.3_Global_.zip\n",
      "TO data/core/gc/Globcover2009_V2.3_Global_.zip\n"
     ]
    }
   ],
   "source": [
    "globcover_url = 'http://due.esrin.esa.int/files/Globcover2009_V2.3_Global_.zip'\n",
    "\n",
    "response = open_urls(globcover_url)\n",
    "write_response(dirs.gc, *response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted the following files ['GLOBCOVER_L4_200901_200912_V2.3.tif', 'GLOBCOVER_L4_200901_200912_V2.3_CLA_QL.tif']\n",
      "Removed archive data/core/gc/Globcover2009_V2.3_Global_.zip\n"
     ]
    }
   ],
   "source": [
    "path_to_zip = dirs.gc + os.sep + os.listdir(dirs.gc)[0]\n",
    "regex = re.compile(r'.+\\.tif', re.IGNORECASE)\n",
    "\n",
    "with zipfile.ZipFile(path_to_zip, 'r') as src:\n",
    "    to_extract = [item for item in src.namelist() if bool(regex.match(item))]\n",
    "    src.extractall(path=dirs.gc, members=to_extract)\n",
    "\n",
    "print('Extracted the following files {}'.format(to_extract))\n",
    "os.remove(path_to_zip)\n",
    "print('Removed archive {}'.format(path_to_zip))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliary data\n",
    "<p><font color='red'>**Not done yet**</font></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "notify_time": "30",
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
