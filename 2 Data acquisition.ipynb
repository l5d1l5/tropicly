{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data acquisition\n",
    "This *Jupyter* notebook comprises the required core and auxiliary data downloads for my master thesis. All downloads are stored locally in a directory tree called ***data*** within the notebooks root directory. Be sure that you have enough space on your hard disk because these downloads need it. Further, you should have a fast web access in order to download the required files quickly. The proceeding notebook is a top down approach, therefore you should execute code cells in a top-down manner. If you execute code cells in a arbitrary order it leads to unrecognized exceptions and errors. So, for your own sake don't do it please. Moreover, it is not checked if you already downloaded the data yet. Hence, executing the code snippets twice will lead to re-download of the entire datasets.\n",
    "\n",
    "My master thesis has the emphasis deforestation and deforestation drivers on global, continental and local scale in the tropical zone between 2001 and 2010. Therefore, the downloads will be filtered in the extent of this zone which covering a latitudinal area between approximately 23.43&deg; North and -23.43&deg; South (WGS84). Furthermore, the data acquisition focuses on datasets of the mentioned time period. This notebook comprises the following sections:\n",
    "\n",
    "[**Preparation**](#Preparation) contains all required initial steps like importing crucial standard library modules and construction of the directory tree for storing the downloaded datasets. The code snippets of this section are fundamental and if you refuse to execute them the proceeding code cells will run into fatal errors.\n",
    "\n",
    "[**Core data**](#Core-data) section encompass the required code cells for downloading and filtering of the core datasets for determining the drivers of deforestation in tropics. These datasets are raster files of the type GeoTIFF (file-extension \\*.tif) which are stored in a sub-folder sorted manner within the directory **core** according to [Section Preparation](#Preparation). As mentioned at the first paragraph you must execute the code cells in the provided top-down order otherwise the execution fails and crucial data will be missing in the proceeding chapters. \n",
    "\n",
    "[**Auxiliary data**](#Auxiliary-data) <font color='red'>**Not done yet**</font> this section should contain extra data downloads like satellite images for NDVI computation, country masks, tile masks etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "As the first step we must import all necessary *Python* standard library modules as well as other mandatory packages to accomplish the data download and filtering successful. In detail the following modules are required:\n",
    "- ***matplotlib*** a ipython built-in magic command for importing matplotlib and to enable the inline backend for notebooks\n",
    "- ***re*** a module for applying regular expressions\n",
    "- ***os*** a module to use operating system dependent functionality\n",
    "- ***zipfile*** a module that provides tools for ZIP archives\n",
    "- ***threading*** a module that provides a multi-threading API\n",
    "- ***urllib.request*** a module to open and download urls\n",
    "- ***geopandas*** a powerful and feature rich package for manipulating geo-vector files like shp, geojson etc. wrapped around pandas and GDAL\n",
    "- ***collections.namedtuple*** a module to create tuple-like objects that have fields accessible by attribute lookup as well as being indexable and iterable\n",
    "- ***IPython.display.clear_output*** a module method to clear the output of an arbitrary code cell "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import re\n",
    "import zipfile\n",
    "import threading\n",
    "import urllib.request\n",
    "import geopandas as gpd\n",
    "from collections import namedtuple\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we create with the following code cell the ***data*** directory tree among the root folder.\n",
    "- **data**\n",
    "    - **core** the entire data from [Section 1.2](#Core-data)\n",
    "        - **gfc** data from [Section 1.2.1](#Global-Forest-Change)\n",
    "        - **gl30** data from [Section 1.2.2](#GlobalLand30)\n",
    "        - **gc** data from [Section 1.2.3](#GlobCover)\n",
    "    - **auxiliary** the entire data from [Section 1.3](#Auxiliary-data)\n",
    "        - **masks** data from [Section 1.2.2](#GlobalLand30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:\tdata File exists\n",
      "Error:\tdata/core File exists\n",
      "Error:\tdata/core/gfc File exists\n",
      "Error:\tdata/core/gl30 File exists\n",
      "Error:\tdata/core/gc File exists\n",
      "Error:\tdata/auxiliary File exists\n",
      "Error:\tdata/auxiliary/masks File exists\n"
     ]
    }
   ],
   "source": [
    "Directories = namedtuple('Directories', 'root core gfc gl30 gc auxiliary masks')\n",
    "\n",
    "# directories to create\n",
    "directories_data = 'data data.core data.core.gfc data.core.gl30 data.core.gc data.auxiliary data.auxiliary.masks'\n",
    "\n",
    "# os compatibility replace \".\" with os dependent path separator and store string in namedtuple Directories\n",
    "dirs = Directories(*re.sub(r'\\.', os.sep, directories_data).split())\n",
    "\n",
    "# make directories according to values assigned to namedtuple Directories\n",
    "for directory in dirs:\n",
    "    try:\n",
    "        os.mkdir(directory)\n",
    "        print('Created:\\t{}'.format(directory))\n",
    "    except OSError as e:\n",
    "        print('Error:\\t{} {}'.format(directory, e.strerror))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core data\n",
    "Explain the phrase core data\n",
    "- pivotal data for the processing pipeline\n",
    "- list the different datasets\n",
    "- explain the filter properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Forest Change\n",
    "[**Global Forest Change 2000-2012 Version 1.0**](https://earthenginepartners.appspot.com/science-2013-global-forest/download_v1.0.html) (GFC) is the first high resolution dataset that provides a comprehensive view on the annual global forest cover change between 2000 and 2012 \\cite{Hansen2013, Li2017}. The initial GFC dataset released by Hansen et al. is extended by recent releases which encompass the annual forest cover changes between [2000-2013 (Version 1.1)](https://earthenginepartners.appspot.com/science-2013-global-forest/download_v1.1.html), [2000-2014 (Version 1.2)](https://earthenginepartners.appspot.com/science-2013-global-forest/download_v1.2.html), [2000-2015 (Version 1.3)](https://earthenginepartners.appspot.com/science-2013-global-forest/download_v1.3.html) and [2000-2016 (Version 1.4)](https://earthenginepartners.appspot.com/science-2013-global-forest/download_v1.4.html) respectively. All versions of this dataset has in common, that they are derived from growing season imagery captured by the remote sensing satellite Landsat 7 Enhanced Thematic Mapper Plus (ETM+) at a spatial resolution of 30 meters per pixel \\cite{Hansen2013a}. On the satellite imagery a time-series spectral metrics analysis is applied to gather the global forest extent at 2000 as well as the annual forest loss and gain. Hence, GFC comprises three independent data layers  tree cover, annually forest loss and  forest gain divided into 10x10 degree tiles by the geodetic coordinate system *World Geodetic System 1984* (EPSG:4326).  \n",
    "\n",
    "[Global Forest Watch](http://www.globalforestwatch.org/) interactive map\n",
    "\n",
    "- Flow general what is gfc then detailed info monitoring method, details of the different layers, how certain is the info\n",
    "- trees defined as all vegetation higher than 5 meters Hansen2013, Hansen2013a\n",
    "- forest loss defined as a stand displacement disturbance (> x% crown cover to 0% crown cover)  Hansen2013, Hansen2013a\n",
    "- monitored by a reference percent tree cover stratum Hansen2013, Hansen2013a\n",
    "- forest degeneration for example selective removals btw. all impacts on forest which are not lead to a non forest state are not considered Hansen2013a\n",
    "- term forest refer to tree cover Hansen2013a\n",
    "- gain is the inverse of loss e.g. the change of a non forest state to forest (crown cover densities >50%)\n",
    "- Forest loss detection is less uncertain then gain detection (loss is more reliable) Li2017\n",
    "- Gain is a more gradual and ecological complex process, signal is more difficult to detect Li2017\n",
    "- Li2017 compares 4 different forest cover change products on their performance to estimate loss and gain patterns in china\n",
    "- at the end show a example picture of the data\n",
    "\n",
    "\\cite{Hansen2013}\n",
    "\\cite{Hansen2013a}\n",
    "\\cite{Tropek2014}\n",
    "\\cite{Bellot2014}\n",
    "\\cite{Li2017}\n",
    "\\cite{Li2017a}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**Code explanation here**</font>\n",
    "- [**Treecover2000**](http://commondatastorage.googleapis.com/earthenginepartners-hansen/GFC2013/treecover2000.txt)\n",
    "- [**Gain**](http://commondatastorage.googleapis.com/earthenginepartners-hansen/GFC2013/gain.txt)\n",
    "- [**Lossyear**](http://commondatastorage.googleapis.com/earthenginepartners-hansen/GFC2013/lossyear.txt)\n",
    "- need url for each tile for each layer\n",
    "- researchers provide a url index file for each lyer\n",
    "- download the relevant files\n",
    "- store content (tile urls) in a variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treecover2000:\t504 URLs\n",
      "Gain:\t\t504 URLs\n",
      "Lossyear:\t504 URLs\n"
     ]
    }
   ],
   "source": [
    "# data source URL\n",
    "head_url = 'http://commondatastorage.googleapis.com/earthenginepartners-hansen/GFC2013/'\n",
    "\n",
    "# append the relevant layer URL index file to head url, download file content and assign it to a list variable\n",
    "treecover_urls = urllib.request.urlopen(head_url + 'treecover2000.txt').read().decode().splitlines()\n",
    "gain_urls = urllib.request.urlopen(head_url + 'gain.txt').read().decode().splitlines()\n",
    "lossyear_urls = urllib.request.urlopen(head_url + 'lossyear.txt').read().decode().splitlines()\n",
    "\n",
    "print('Treecover2000:\\t{} URLs\\nGain:\\t\\t{} URLs\\nLossyear:\\t{} URLs'\n",
    "      .format(len(treecover_urls), len(gain_urls), len(lossyear_urls)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**Code explanation here**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288 URLs are not in bounds between 30 North and -20 South.\n",
      "Filtered URLs contains 3 * 216 elements.\n"
     ]
    }
   ],
   "source": [
    "def is_in_extent(coord: int, orient: str, north_limit: int, south_limit: int):\n",
    "    if orient.lower() == 'n':\n",
    "        return coord <= north_limit\n",
    "    elif orient.lower() == 's':\n",
    "        return coord <= south_limit\n",
    "    return False\n",
    "\n",
    "# http://commondatastorage.googleapis.com/earthenginepartners-hansen/GFC2013/Hansen_GFC2013_lossyear_00N_030W.tif\n",
    "regex = re.compile(r\"\"\"\n",
    "                        (?:\\w+_){3}  # supress group consumption, alphanumeric char one or more times (greedy) followed by underline, match group three times  \n",
    "                        (?P<coord>\\d{2})  # named group consumption, digit two times\n",
    "                        (?P<orient>S|N)  # named group consumption, S or N\n",
    "                    \"\"\", re.VERBOSE)\n",
    "\n",
    "# [(treecover, gain, lossyear), ...]\n",
    "raw_urls = list(zip(treecover_urls, gain_urls, lossyear_urls))\n",
    "filtered_urls = []\n",
    "\n",
    "for urls in raw_urls:\n",
    "    coord, orient = regex.search(urls[0]).groups()\n",
    "    if is_in_extent(int(coord), orient, 30, 20):\n",
    "        filtered_urls.append(urls)\n",
    "\n",
    "print('{} URLs are not in bounds between {} North and {} South.'\n",
    "      .format(len(raw_urls) - len(filtered_urls), 30, -20))\n",
    "print('Filtered URLs contains 3 * {} elements.'.format(len(filtered_urls)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**Code explanation here**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOWNLOADED 648 OF 648 FILES\n"
     ]
    }
   ],
   "source": [
    "def open_urls(*args):\n",
    "    response = [urllib.request.urlopen(url) for url in args]\n",
    "    return response\n",
    "\n",
    "\n",
    "def write_response(out_path: str, *args):\n",
    "    for response in args:\n",
    "        filename = response.url.split('/')[-1]\n",
    "        with open(os.path.join(out_path, filename), 'wb') as dst:\n",
    "            dst.write(response.read())\n",
    "        print('DOWNLOADED {}\\nTO {}'.format(response.url, os.path.join(out_path, filename)))\n",
    "\n",
    "\n",
    "def worker(out_path: str, urls: list):\n",
    "    response = open_urls(*urls)\n",
    "    write_response(out_path, *response)\n",
    "\n",
    "\n",
    "finished = 0\n",
    "for idx in range(0, len(filtered_urls), 3):\n",
    "    current_urls = filtered_urls[idx:idx + 3]\n",
    "    threads = []\n",
    "    for urls in current_urls:\n",
    "        thread = threading.Thread(target=worker, args=(dirs.gfc, urls))\n",
    "        thread.start()\n",
    "        threads.append(thread)\n",
    "    [thread.join() for thread in threads]\n",
    "    clear_output()\n",
    "    finished += len(current_urls) * 3\n",
    "    print('DOWNLOADED {} OF {} FILES'.format(finished, len(filtered_urls) * 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Hansen preview](img/hansen_preview.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GlobalLand30\n",
    "\n",
    "[GL30 mask](http://globallandcover.com/document/globemapsheet.zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted the following files ['GlobeMapSheet.dbf', 'GlobeMapSheet.prj', 'GlobeMapSheet.sbn', 'GlobeMapSheet.sbx', 'GlobeMapSheet.shp', 'GlobeMapSheet.shp.xml', 'GlobeMapSheet.shx']\n",
      "Removed archive data/auxiliary/masks/globemapsheet.zip\n"
     ]
    }
   ],
   "source": [
    "gl30_mask_url = 'http://globallandcover.com/document/globemapsheet.zip'\n",
    "gl30_mask_path = os.path.join(dirs.masks, gl30_mask_url.split('/')[-1])\n",
    "\n",
    "with zipfile.ZipFile(gl30_mask_path, 'r') as src:\n",
    "    to_extract = src.namelist()\n",
    "    src.extractall(path=dirs.masks)\n",
    " \n",
    "\n",
    "print('Extracted the following files {}'.format(to_extract))\n",
    "os.remove(gl30_mask_path)\n",
    "print('Removed archive {}'.format(gl30_mask_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "495 Tiles are not in bounds between 23.43 North and -23.43 South.\n",
      "File filter contains 358 elements.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NS</th>\n",
       "      <th>UTMZONE</th>\n",
       "      <th>ROW</th>\n",
       "      <th>CONTINENT</th>\n",
       "      <th>REMARK</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>S</td>\n",
       "      <td>48</td>\n",
       "      <td>10</td>\n",
       "      <td>Asia</td>\n",
       "      <td>S48_10</td>\n",
       "      <td>POLYGON ((108 -15.00000003, 101.99999988 -15.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>S</td>\n",
       "      <td>50</td>\n",
       "      <td>10</td>\n",
       "      <td>Asia</td>\n",
       "      <td>S50_10</td>\n",
       "      <td>POLYGON ((119.99999988 -15.00000003, 114.00000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>S</td>\n",
       "      <td>43</td>\n",
       "      <td>5</td>\n",
       "      <td>Asia</td>\n",
       "      <td>S43_5</td>\n",
       "      <td>POLYGON ((78.00000012 -9.999999989999999, 72 -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>S</td>\n",
       "      <td>48</td>\n",
       "      <td>5</td>\n",
       "      <td>Asia</td>\n",
       "      <td>S48_5</td>\n",
       "      <td>POLYGON ((108 -9.999999989999999, 101.99999988...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>S</td>\n",
       "      <td>49</td>\n",
       "      <td>5</td>\n",
       "      <td>Asia</td>\n",
       "      <td>S49_5</td>\n",
       "      <td>POLYGON ((114.00000012 -9.999999989999999, 108...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   NS  UTMZONE  ROW CONTINENT  REMARK  \\\n",
       "37  S       48   10      Asia  S48_10   \n",
       "38  S       50   10      Asia  S50_10   \n",
       "39  S       43    5      Asia   S43_5   \n",
       "40  S       48    5      Asia   S48_5   \n",
       "41  S       49    5      Asia   S49_5   \n",
       "\n",
       "                                             geometry  \n",
       "37  POLYGON ((108 -15.00000003, 101.99999988 -15.0...  \n",
       "38  POLYGON ((119.99999988 -15.00000003, 114.00000...  \n",
       "39  POLYGON ((78.00000012 -9.999999989999999, 72 -...  \n",
       "40  POLYGON ((108 -9.999999989999999, 101.99999988...  \n",
       "41  POLYGON ((114.00000012 -9.999999989999999, 108...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gl30_mask_shp = [item for item in os.listdir(dirs.masks) if bool(re.match(r'.+\\.shp$', item))][0]\n",
    "\n",
    "gl30_mask = gpd.read_file(os.path.join(dirs.masks, gl30_mask_shp)).cx[:,-23.43:23.43]\n",
    "gl30_file_filter = [\n",
    "    '_'.join([items[0] + str(items[1]).zfill(2), str(items[2]).zfill(2)])\n",
    "    for items in zip(gl30_mask.NS, gl30_mask.UTMZONE, gl30_mask.ROW)\n",
    "                   ]\n",
    "\n",
    "print('{} Tiles are not in bounds between {} North and {} South.'\n",
    "      .format(853 - len(gl30_mask), 23.43, -23.43))\n",
    "print('File filter contains {} elements.'.format(len(gl30_file_filter)))\n",
    "gl30_mask.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unzip:\tdata/core/gl30/S04_00_2010LC030/s04_00_2010lc030.tif\n",
      "Move:\tdata/core/gl30/s04_00_2010lc030.tif\n",
      "Removed:\tdata/core/gl30/S04_00_2010LC030.zip\n"
     ]
    }
   ],
   "source": [
    "regex_id = re.compile(r'(?P<id>(?:N|S)\\d{2}_\\d{2})', re.VERBOSE)\n",
    "regex_ext = re.compile(r'.+\\.tif', re.VERBOSE)\n",
    "\n",
    "count = 0\n",
    "for item in os.listdir(dirs.gl30):\n",
    "    match = regex_id.search(item).group('id')\n",
    "    if match not in gl30_file_filter:\n",
    "        os.remove(os.path.join(dirs.gl30, item))\n",
    "    else:\n",
    "        with zipfile.ZipFile(os.path.join(dirs.gl30, item)) as src:\n",
    "            to_extract = [ele for ele in src.namelist() if bool(regex_ext.match(ele))][0]\n",
    "            src.extract(to_extract, path=dirs.gl30)\n",
    "        os.rename(os.path.join(dirs.gl30, to_extract), dirs.gl30 + os.sep + to_extract.split(os.sep)[-1])\n",
    "        os.rmdir(dirs.gl30 + os.sep + to_extract.split(os.sep)[0])\n",
    "        os.remove(os.path.join(dirs.gl30, item))\n",
    "        clear_output()\n",
    "        print('Unzip:\\t{}'.format(os.path.join(dirs.gl30, to_extract)))\n",
    "        print('Move:\\t{}'.format(os.path.join(dirs.gl30, to_extract.split(os.sep)[-1])))\n",
    "    print('Removed:\\t{}'.format(os.path.join(dirs.gl30, item)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Chen preview](img/chen_preview.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GlobCover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOWNLOADED http://due.esrin.esa.int/files/Globcover2009_V2.3_Global_.zip\n",
      "TO data/core/gc/Globcover2009_V2.3_Global_.zip\n"
     ]
    }
   ],
   "source": [
    "globcover_url = 'http://due.esrin.esa.int/files/Globcover2009_V2.3_Global_.zip'\n",
    "\n",
    "response = open_urls(globcover_url)\n",
    "write_response(dirs.gc, *response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted the following files ['GLOBCOVER_L4_200901_200912_V2.3.tif', 'GLOBCOVER_L4_200901_200912_V2.3_CLA_QL.tif']\n",
      "Removed archive data/core/gc/Globcover2009_V2.3_Global_.zip\n"
     ]
    }
   ],
   "source": [
    "path_to_zip = os.path.join(dirs.gc, os.listdir(dirs.gc)[0])\n",
    "regex = re.compile(r'.+\\.tif', re.IGNORECASE)\n",
    "\n",
    "with zipfile.ZipFile(path_to_zip, 'r') as src:\n",
    "    to_extract = [item for item in src.namelist() if bool(regex.match(item))]\n",
    "    src.extractall(path=dirs.gc, members=to_extract)\n",
    "\n",
    "print('Extracted the following files {}'.format(to_extract))\n",
    "os.remove(path_to_zip)\n",
    "print('Removed archive {}'.format(path_to_zip))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ecosystem service valuation database\n",
    "https://www.es-partnership.org/services/data-knowledge-sharing/ecosystem-service-valuation-database/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliary data\n",
    "<p><font color='red'>**Not done yet**</font></p>\n",
    "- country mask http://gadm.org/\n",
    "- http://biogeo.ucdavis.edu/data/gadm2.8/gadm28_levels.shp.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result\n",
    "<p><font color='red'>**Not done yet**</font></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### References\n",
    "\n",
    "[<a id=\"cit-Hansen2013\" href=\"#call-Hansen2013\">1</a>] C. M., V. P., Moore R. <em>et al.</em>, ``_High-Resolution Global Maps of 21st-Century Forest Cover Change_'', Science, vol. 342, number 6160, pp. 850--853, November 2013.\n",
    "\n",
    "[<a id=\"cit-Hansen2013a\" href=\"#call-Hansen2013a\">2</a>] C. M., V. P., Moore R. <em>et al.</em>, ``_Supplementary Materials for: High-Resolution Global Maps of 21st-Century Forest Cover Change_'', Sciene, vol. 342, number 6160, pp. 1--32, November 2013.  [online](http://science.sciencemag.org/content/suppl/2013/11/14/342.6160.850.DC1)\n",
    "\n",
    "[<a id=\"cit-Tropek2014\" href=\"#call-Tropek2014\">3</a>] Tropek Robert, Sedl{\\'{a}}{\\v{c}}ek Ond{\\v{r}}ej, Beck Jan <em>et al.</em>, ``_Comment on High-resolution global maps of 21st-century forest cover change_'', Science, vol. 344, number 981, pp. ,  2014.\n",
    "\n",
    "[<a id=\"cit-Bellot2014\" href=\"#call-Bellot2014\">4</a>] Bellot Franz-Fabian, Bertram Mathias, Navratilb Peter <em>et al.</em>, ``_The high-resolution global map of 21st-century forest cover change from the University of Maryland (Hansen Map) is hugely overestimating deforestation in Indonesia_'', FORCLIME Press release, vol. , number , pp. ,  2014.  [online](http://www.forclime.org/documents/press_release/FORCLIME_Overestimation%20of%20Deforestation.pdf)\n",
    "\n",
    "[<a id=\"cit-Li2017\" href=\"#call-Li2017\">5</a>] Li Yan, Sulla-Menashe Damien, Motesharrei Safa <em>et al.</em>, ``_Inconsistent estimates of forest cover change in China between 2000 and 2013 from multiple datasets: differences in parameters, spatial resolution, and definitions_'', Scientific Reports, vol. 7, number 8748, pp. , August 2017.\n",
    "\n",
    "[<a id=\"cit-Li2017a\" href=\"#call-Li2017a\">6</a>] Li Yan, Sulla-Menashe Damien, Motesharrei Safa <em>et al.</em>, ``_Supplementary Information for Inconsistent estimates of forest cover change in China between 2000 and 2013 from multiple datasets_'', Scientific reports, vol. 7, number 8748, pp. , August 2017.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "lit.bib",
   "cite_by": "number",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "notify_time": "30",
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
