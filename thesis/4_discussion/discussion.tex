\chapter{Discussion}
\label{ch:discussion}
	\section{Proximate deforestation drivers}
	\label{sec:discussion_deforestation}

		\subsection{Forest definition}
		\label{subsec:discussion_forest_definition}
			\begin{itemize}
				\item For a regional approach a better solution could be to select for each region independently the right canopy density. For America a good agreement between the tree cover could be achieved by selecting the second class. For Asia by selecting the first class and for africa the second. Even better would be to decide per tile individually which canopy density should be selected. This would eliminate regional effects of different forest densities.
				\item discuss regions independently asia and america have large tree cover agreement
				\item africa has the lowest agreement only core forest zones show high similarity
				\item To improve we should apply for each tile a canopy class decision based on our analysis
				\item This could improve the improve the similarity (accuracy) by maximizing the sample count
				\item Extent method by analyzing the differences per clustering
				\item Could be used for \citeauthor{Sannier2016} method as precursor to identify canopy densities which are worth for a detailed investigation
				\item Algorithm draft for single similarity: Compute jaccard indexes for tile pair at different canopy densities, put results in a list, sort the list in decreasing order, pick the class where jaccard index is max
				\item Use this jaccard method to exclude tiles where the tree cover similarity fall below a certain threshold
			\end{itemize}

		\subsection{Tree cover and deforestation patterns}
		\label{subsec:discussion_tree_cover_and_deforestation}
			\begin{itemize}
				\item Improved tree cover map divide tree cover within hexagon by landmass within hexagon
				\item Improved loss map divide loss by the tree cover within a hexagon
			\end{itemize}

		\subsection{Mapping of proximate deforestation driver}
		\label{subsec:discussion_proxy_deforestation_driver}
			\begin{itemize}
				\item Reclassification is a good idea but not by the chosen approach because the probability is higher that it get assigned the label for the most common classes 
			\end{itemize}

		\subsection{Accuracy assessment}
		\label{subsec:discussion_accuracy_assessment}
			\begin{itemize}
				\item Should be done by independent person was not the case
				\item Should be improved by using approach discussed in \citep{Olofsson2014}
				\item Do shit talk on kappa is not required
				\item Check how many losses could be explained by the \ac{GL30} layer
			\end{itemize}

	\section{Emissions}

	\section{Ecosystem service values}
		\begin{itemize}
			\item resilience of esv loss could be achieved over optimizing total value of the new land-use
			\item target optimization is use the clearcut by maximizing profit and minimizing the esv loss
			\item large differences between the datasets
			\item till now the most complete data for global estimates is \citeauthor{Costanza2014}
			\item \citeauthor{Groot2012} global values are redundant because nearly same estimate for tropical forest and smaller number of biomes is represented by this dataset
			\item \citeauthor{Siikamaki2015} provides forest values which consider only a small amount of ecosystem services but it provides for each country a individual value
		\end{itemize}

	\section{Binning analysis and visualization}
		We build our own algorithm to aggregate raster data by hexagonal binning. We developed an approach to present multivariate raster data by hexagonal binning in connection with a hexa pie chart and choropleth mapping.

		What is good:
		We show that hexagonal binning and a appropriate scaling can highlight spatial patterns as deforestation and tree cover maps show. Further the extended version of our hexagons can even show this patterns for multivariate data with more than 2 dimensions.

		Improvements:
		For the binning we decided to shift the initial hexagon to the point of origin and then translate each grid cell to it position by affine transformation. This was a bad decision because this transformation requires more operations than creating hexagon at its required position. Just create a hexagon at point of origin and then do vector addition with required midpoint. The piechart generation by our analytic solution dont generalize really well it works only for pointy hexagons. Scala presents an approach for cutting lines by an convex window. This approach could be modified to cut convex polygons. Problem of both 10\% are not 10\% of the hexagon area because we use y range to determine relative area. Hexagon have not equal area at each point of y. A better solution is to use scaling if 20 just scale polygon 20 percent smaller.

